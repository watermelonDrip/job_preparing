# AMP

AMP 最开始用在压缩感知上。后来AMP用在标准线性回归上。

标准线性回归问题是从噪声线性观测值Y= AX0+W恢复向量X0。
Donoho、Maleki和Muntali提出的近似消息传递（AMP）算法是一种计算效率很高的迭代方法，它对SLR具有显著的性质：对于大I.I.D.亚高斯矩阵A，其每次迭代行为的严格特征是标量状态演化，其不动点在唯一时是Bayes最优的。
然而，AMP算法是脆弱的，因为即使是与i.i.d.亚高斯模型的微小偏差也会导致算法发散。提出了UAMP算法， 其中U表示的是酉变换。从实验室和理论证明了UAMP 适用于更广泛的一类大随机矩阵a, 特别是对于 
行相关性比较强的矩阵 或者欠秩的矩阵， 收敛速度快并且稳定。

# SBL
 
稀疏贝叶斯学习(Sparse Bayesian Learning, SBL)最初作为一种机器学习算法由 Tipping 于 2001 年 前后提出[Tipping2001]，
随后被引入到稀疏信号恢复/压缩感知领域[Wipf2004,Ji2008]。刚刚说对于矩阵不好的时候效果不好，SBL 表现很好。因此，在雷达追踪，波达方向估计， 脑源定位，特征提取，功率谱估计等一些列领域，SBL 都具备显著的优势。

作为一种贝叶斯算法，SBL 算法对利用这些解的结构信息 提供了更多的灵活性。这种灵活性最主要来自于 SBL 采用参数化的高斯分布为解的先验分布。 

SBL 假设x中的每个元素都服从一个参数化的均值为0，方差为gamma_i的高斯分布，其中这个gamma_i是未知的，但是可以通过算法自动估计出来。

在运算中，绝大部分的gamma_i变成0，或者接近0。SBL 通常会采用一个阈值趋近于0的gamma_i置于0，但gamma_i =0，x=0。
因此，gamma_i 与解的稀疏程度密切相关。在以上的SBL 框架中，我们吧gamma_i作为未知的确定性参数，而没有把它视为一个随机变量
从而进一步假设它的先验分布。可以选取不同的分布作为先验。绝大多数SBL算法对噪声方差 λ 的估计都不有效，尤其是当感知矩阵的列与列之间 具有强相关性且噪声很大的时候。而对该方差估计的准确性对 x 的估计的准确性影响非常大。


 
