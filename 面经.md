1. ROC 曲线的纵轴是"真正 例率" (True Positive Rate，简称 TPR=tp/(tp+fn))，横轴是"假正例率" (False Positive Rate，简称 FPR=tp/(tn+fp))，
2. 解决过拟合的办法：特征降维,添加正则化，降低模型的复杂度,Dropout,Early stopping
3. L1正则化（也叫Lasso回归）是在目标函数中加上与系数的绝对值相关的项，而L2正则化（也叫岭回归）则是在目标函数中加上与系数的平方相关的项。当有多个相关的特征时,Elastic Net回归
同时将 L1 和 L2 正则化项应用于线性回归的损失函数。
4. 贝叶斯好好的，为什么要加上一个naive呢？因为需要假设所有特征都是相互独立的。什么样的特征是独立特征呢？说出朴素⻉贝叶斯模型的构建过程以及预测过程.



 5. 朴素贝叶斯是一种典型的生成式模型，
 朴素贝叶斯算法的基本思想是建立特征 x与输出y 之间的联合概率分布  ，在对给定的特征进行预测时，通过贝叶斯定理求出所有可能的输出后验概率 ，取其中最大的作为预测结果。其优点是模型简单，效率高，在很多领域有广泛的使用。
 有监督学习可以分为两类：判别模型和生成模型，我们所熟悉的神经网络，支持向量机和logistic regression，决策树等都是判别模型。而朴素贝叶斯和隐马尔可夫模型则属于生成式模型
 
 6. 生成式模型先对数据的联合分布 进行建模，然后再通过贝叶斯公式计算样本属于各类别的后验概率 。

判别式模型直接进行条件概率建模，由数据直接学习决策函数 或条件概率分布 作为预测的模型。判别方法不关心背后的数据分布，关心的是对于给定的输入，应该预测什么样的输出。

7. 决策树与随机森林的区别是什么？ 如果让你选择，你会使⽤哪个模型，为什么？
+ 决策树和随机林的主要区别在于，决策树是一个使用分支方法来说明决策的每个可能结果的图，而随机林是一组基于其所有决策树的输出给出最终结果的决策树。
+ 决策树是一种有监督的机器学习算法，该方法可以用于解决分类和回归问题。决策树可以简单地理解为达到某一特定结果的一系列决策。
+  一棵简单的树并不能产生有效的结果。这就是随机森林算法的用武之地。随机森林是基于树的机器学习算法，该算法利用了多棵决策树的力量来进行决策。顾名思义，它是由一片树木组成的“森林”！。
这种结合了多个单一模型的输出（也被称为弱学习）的过程被称为集成学习。
+决策树中存在过度拟合的可能性。在随机林中使用多棵树可以降低过度拟合的风险。决策树比随机林更简单、更容易理解、解释和可视化，而随机林相对来说更复杂。

8. 什么叫 EM 算法？有哪些经典模型的求解过程会⽤到 EM 算法？

最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。

用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

9.LR和SVM的异同点


10. EM 算法是否⼀定会收敛？EM 算法给出的全局最优还是局部最优？


EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。


11. 数据集拥有⾮常多的特征，但样本个数有限，所以计划做特征选择，有哪些⽅法 可以⽤来做特征选择呢

过滤法（Filter）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征
包裹法（Wrapper）：根据目标函数，每次选择若干特征或者排除若干特征，直到选择出最佳的子集。
嵌入法（Embedding）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
 
12. 



