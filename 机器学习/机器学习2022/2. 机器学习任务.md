# issues
Model Bias: 
+ the model is too simple
+ solution: redesign your model to make it more flexible

Optimization issue:

Solution: 1.  gaining the insights from comparison 2.从小而简单的网络开始

overfitting （model overfitting) 1. 增加训练资料（data augmentation) 2. 给模型一些限制（少神经元，model 公用参数）3. less features 4. early stopping 5. regularization 6. dropout

cross validation

mismatch : 训练资料，测试资料分布不同（要理解资料）

# when gradients is small

issue: training loss一直不变小，或者不够小。

critical point : 微分= 0
+ local minima , H : all eigen values are positive
+ local maxima, H: negative
+ saddle point: H: some postive,some negative
 
 H: 指出update 方向（ theta = theta'+u (u eigen vector,对应eigen value <0))
 
 ## Batch size(hyperparameter)
 shuffle: 每次shuffle, 每个batch 不一样
 大Batch:  利用GPU 平行运算，快，稳定
 小Batch:  噪声大，准确，慢，不容易卡住，better on test data, better for training

小batch size 结果好，大的batch size 有opimization 问题

大batch 好，小batch 差，是overfitting

flat minima: 好minima (small batch size)

sharp minima: 坏minima （large batch size)

 ## momentum  (hyperparameter)
 
 梯度方向和上一次迭代的方向的一个折中。每次的方向和之前的都有关。
 
 
 # Adaptive learning rate(Error surface)
 
 critical point 不一定是训练时候最大的阻碍。
 
 
 梯度越大，learning rate变换越小，因为是求和，所以迭代次数越多，learning rate变化越小。
 
 ## RMSProp
 
 ## learning rate scheduling 
  1. learning rate decay
  2. warm up
 

## classification

class as one-hot vector

softmax: 把任何值 变成 0到1之间

loss function: cross entropy(minimize cross entropy = maximize likelihood)

cross entropy 在pytorch 里有softmax.

classfication 用 cross-entropy好

# 宝可梦分析器

## 第一步：找一个带未知参数的函数

f()

｜H|代表的是模型的复杂程度，也就是可选择性多

## 定义一个loss function(give data)

+ given a dataset D
+ loss of a threshold h given data set D

## optimization

testing data D_test as the proxy of D_all

sample 到好的资料，就会和现实很接近： L(h_train,D_all)-L(h_all,D_all)< delta 

P.S.  L(h_train,D_train) can be smaller than L(h_all,D_all)

## what kind of D_train fulfil it?

good sample D_train:|L(h,D_all)-L(h,D_all)|< epsilon (epsilon = delta/2) (任何h)

bad sample D_train:|L(h,D_all)-L(h,D_all)|> epsilon (epsilon = delta/2) (至少一个h）

最好是小H，大N

