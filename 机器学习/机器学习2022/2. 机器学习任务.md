# issues
Model Bias: 
+ the model is too simple
+ solution: redesign your model to make it more flexible

Optimization issue:

Solution: 1.  gaining the insights from comparison 2.从小而简单的网络开始

overfitting （model overfitting) 1. 增加训练资料（data augmentation) 2. 给模型一些限制（少神经元，model 公用参数）3. less features 4. early stopping 5. regularization 6. dropout

cross validation

mismatch : 训练资料，测试资料分布不同（要理解资料）

# when gradients is small

issue: training loss一直不变小，或者不够小。

critical point : 微分= 0
+ local minima , H : all eigen values are positive
+ local maxima, H: negative
+ saddle point: H: some postive,some negative
 
 H: 指出update 方向（ theta = theta'+u (u eigen vector,对应eigen value <0))
 
 ## Batch size(hyperparameter)
 shuffle: 每次shuffle, 每个batch 不一样
 大Batch:  利用GPU 平行运算，快，稳定
 小Batch:  噪声大，准确，慢，不容易卡住，better on test data, better for training

小batch size 结果好，大的batch size 有opimization 问题

大batch 好，小batch 差，是overfitting

flat minima: 好minima (small batch size)

sharp minima: 坏minima （large batch size)

 ## momentum  (hyperparameter)
 
 梯度方向和上一次迭代的方向的一个折中。每次的方向和之前的都有关。
 
 
 # Adaptive learning rate(Error surface)
 
 critical point 不一定是训练时候最大的阻碍。
 
 
 梯度越大，learning rate变换越小，因为是求和，所以迭代次数越多，learning rate变化越小。
 
 ## RMSProp
 
 ## learning rate scheduling 
  1. learning rate decay
  2. warm up
 

